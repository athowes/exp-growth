---
title: "Benchmarking Poisson regression for exponential growth detection"
author:
- name: Adam Howes
output:
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
    df_print: paged
    code_folding: hide
    theme: lumen
abstract: |
    **Background** Poisson regression is a candidate method for exponential growth detection. The number of k-mers a realistic Nucleic Acid Observatory would process is large. For this reason, it's important that any candidate methods are assessed for their computational feasibility at scale.
    
    **Task** We will write an R script for performing Poisson regression exponential growth detection on real data. Using code profling and benchmarking tools, we will iterate on the code to improve its computational performance. For the final code, we will produce a summary of the runtime for each line. We will repeat the runtime analysis for varying total numbers of k-mers. We will output an estimated time per e.g. million k-mers.
    
    **Findings** 
    
    **Next steps**
---

```{r setup, class.source = 'fold-hide'}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  autodep = TRUE,
  cache.lazy = FALSE,
  cache.comments = FALSE
)

options(scipen = 999)

cbpalette <- multi.utils::cbpalette()

library(tidyverse)
```

## Resources which might be useful

* [23 Measuring performance, Advanced R](https://adv-r.hadley.nz/perf-measure.html)
* [25 Many models, R for Data Science](https://r4ds.had.co.nz/many-models.html)
* [Whassup with `glm()`?, Gelman](https://statmodeling.stat.columbia.edu/2011/05/04/whassup_with_gl/)
* [Handling errors using purrr's possibly() and safely()](https://aosmith.rbind.io/2020/08/31/handling-errors/)

## Data

We use the `counts` data for this task.

```{r}
counts <- read.table("data/counts-1pct-sample.tsv", header = TRUE)
subset_length <- 100
subset_counts <- head(counts, n = subset_length)
head(subset_counts)
```
For development of the initial script, we subset `counts` data, which has `r nrow(counts)` rows and is of size `r format(object.size(counts), units = "auto")`, to just the first `r subset_length` rows of size `format(object.size(subset_counts), units = "auto")`.

## Developing an initial script

### `profvis`

```{r}
possibly_glm <- possibly(.f = glm, otherwise = NULL)
possibly_tidy <- possibly(.f = broom::tidy, otherwise = NULL)
```

To begin with, we will use `tidyr::pivot_longer` and `stats::glm` to create a function `f` which we can input into `profvis::profvis` to produce a HTML output.

```{r}
f <- function(df) {
  df_long <- df %>%
  pivot_longer(
    cols = starts_with("day"),
    names_to = "day",
    names_prefix = "day",
    values_to = "count",
    names_transform = list(count = as.integer, day = as.integer)
  )

  fits <- tibble(ec = df$ec)
  fits <- mutate(fits, fit = map(ec, ~possibly_glm(count ~ 1 + day, family = "poisson", data = filter(df_long, ec == .))))
  fits <- mutate(fits, fit = map(fit, possibly_tidy, conf.int = TRUE))
  fits <- unnest(fits, fit)
  return(fits)
}

profvis::profvis(f(subset_counts))
```

### `bench`

What about with `bench::mark` instead:

> Benchmark a list of quoted expressions. Each expression will always run at least twice, once to measure the memory allocation and store results and one or more times to measure timing.

The number of times that each expression is run is determined by... [TODO].

#### Pivoting

```{r}
nrows_small <- 10^{1:3}
nrows_big <- 10^{1:6}

pivot_bm <- bench::press(
  nrows = nrows_big,
  {
    df <- head(counts, n = nrows)
    bench::mark(
      pivot = pivot_longer(
        df,
        cols = starts_with("day"),
        names_to = "day",
        names_prefix = "day",
        values_to = "count",
        names_transform = list(count = as.integer, day = as.integer)
      )
    )
  }
)

pivot_bm

ggplot2::autoplot(pivot_bm)
```

#### Running the generalised linear model

```{r}
glm_bm <- bench::press(
  nrows = nrows_small,
  {
    df <- head(counts, n = nrows)
    df_long <- pivot_longer(
        df,
        cols = starts_with("day"),
        names_to = "day",
        names_prefix = "day",
        values_to = "count",
        names_transform = list(count = as.integer, day = as.integer)
      )
    
    bench::mark(
      glm = {
        fits <- tibble(ec = df$ec)
        fits <- mutate(fits, fit = map(ec, ~possibly_glm(count ~ 1 + day, family = "poisson", data = filter(df_long, ec == .))))
      }
    )
  }
)

glm_bm

ggplot2::autoplot(glm_bm)
```

#### Tidying the generalised linear model

* The R code for `tidy.glm` is given here: https://github.com/tidymodels/broom/blob/520a933cd84974c42ec3ba8df9a37c59634ce211/R/stats-glm-tidiers.R

```{r}
before_tidy <- function(nrows) {
  df <- head(counts, n = nrows)
  df_long <- pivot_longer(
    df,
    cols = starts_with("day"),
    names_to = "day",
    names_prefix = "day",
    values_to = "count",
    names_transform = list(count = as.integer, day = as.integer)
  )
  fits <- tibble(ec = df$ec)
  fits <- mutate(fits, fit = purrr::map(ec, ~possibly_glm(count ~ 1 + day, family = "poisson", data = filter(df_long, ec == .))))
  return(fits)
}

tidy_bm <- bench::press(
  nrows = nrows_small,
  {
    fits <- before_tidy(nrows = nrows)
    
    bench::mark(
      tidy = {
        mutate(fits, fit = purrr::map(fit, ~possibly_tidy))
      }
    )
  }
)

tidy_conf_bm <- bench::press(
  nrows = nrows_small,
  {
    fits <- before_tidy(nrows = nrows)
    
    bench::mark(
      tidy_conf = {
        mutate(fits, fit = purrr::map(fit, ~possibly_tidy, conf.int = TRUE))
      }
    )
  }
)

tidy_bm
tidy_conf_bm

ggplot2::autoplot(tidy_bm)
ggplot2::autoplot(tidy_conf_bm)
```

## Ideas and notes for continuation of this document

* Use `?glm` to check what the possible input formats are
  * Could use `filter` (or something more computationally efficient like `[]`) to pick out rows then put them into `glm`
  * Could "hard-code" in the time covariate, as it is the same for every time-series
* Use data.table to do the pivot, how much quicker is that?
* On the order of GB memory is starting to be prohibitive
* What exactly does `mem_alloc` refer to?
* Try forecasting the requirements for each step at different numbers of $k$-mers (just by extrapolation)
* Is `tidy` the slow running thing? Try it without `conf.int = TRUE`
* Is there a way to specify one call to `glm` with inputs `ec` within the formula and gets what we're aiming for?
* Test running `furrr` 
