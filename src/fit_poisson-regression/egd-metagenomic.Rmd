---
title: "Exponential growth detection on metagenomic data"
author:
- name: Adam Howes
output:
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
    df_print: paged
    code_folding: hide
    theme: lumen
abstract: |
    **Background** 
    
    **Task** 
    
    **Findings** 
    
    **Next steps** 
---

```{r setup, class.source = 'fold-hide'}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  autodep = TRUE,
  cache.comments = FALSE
)

options(scipen = 999)

cbpalette <- multi.utils::cbpalette()

library(tidyverse)
```

## First simulated example

We start by importing the data generated in the notebook [Simulating metagenomic time series data](https://athowes.github.io/exp-growth/metagenomic-time-series).

```{r}
df <- readRDS("depends/sample0.rds") %>%
  as_tibble()
```

This data looks as follows.

```{r}
sample_summary <- df %>%
  group_by(day, regime) %>%
  summarise(
    count_upper = quantile(count, 0.95),
    count_median = median(count),
    count_lower = quantile(count, 0.05)
  )

ggplot(sample_summary, aes(x = day, ymin = count_lower, y = count_median, ymax = count_upper, group = regime)) +
    geom_ribbon(alpha = 0.1, aes(fill = regime)) +
    geom_line(aes(col = regime), size = 1.5) +
    geom_line(data = df, aes(x = day, y = count, col = regime, group = id),
               alpha = 0.025, inherit.aes = FALSE) + 
    theme_minimal() +
    scale_color_manual(values = cbpalette) +
    scale_fill_manual(values = cbpalette) +
    guides(fill = "none") +
    labs(x = "Day", y = "Number of reads in sample", col = "Regime")
```

### Performing inference

We fit a Poisson regression model (see [Explaining Poisson regression](https://athowes.github.io/exp-growth/explain-poisson) or [Estimating exponential growth via sporadic detection](https://athowes.github.io/exp-growth/exponential-detection)) to each $k$-mer in the data.

```{r}
#' Fit to each unique id
#' Note that (confusingly) kmer is not unique, and represents the kmer within a given organism
fits <- tibble(id = unique(df$id)) %>%
  mutate(
    fit = map(id, ~glm(count ~ 1 + day, family = "poisson", data = filter(df, id == .))),
    fit = map(fit, broom::tidy, conf.int = TRUE)
  ) %>%
  unnest(fit)

#' Add truth column
fits <- fits %>%
  left_join(
    select(df, id, regime),
    by = "id"
  )
```

Now we can look at the inference for the slope parameter $\beta$ in each regression, first the point estimate with confidence interval, then the $p$-value for $\beta \neq 0$.
The truth -- either exponential or baseline regime -- is shown by the point colour.

```{r}
fits_day <- filter(fits, term == "day")

fits_day %>%
  ggplot(aes(x = id, y = estimate, ymin = conf.low, ymax = conf.high, col = regime)) +
    geom_pointrange(alpha = 0.05, size = 0.5) +
    theme_minimal() +
    scale_color_manual(values = cbpalette) +
    labs(x = "k-mer ID", y = "Estimated slope", col = "True regime") +
    theme(
      legend.position = "bottom"
    )

fits_day %>%
  ggplot(aes(x = id, y = p.value, col = regime)) +
  geom_point() +
  theme_minimal() +
  scale_color_manual(values = cbpalette) +
  labs(x = "k-mer ID", y = "p-value for positive slope", col = "True regime") +
  theme(
    legend.position = "bottom"
  )

saveRDS(fits, "fits-sample0.rds")
```

### Performance assessment

Suppose we classify as exponential growth when the 95% confidence interval for $\beta$ is above zero.
We can see which $k$-mers we misclassify by comparing this estimated regime to the true regime.

```{r}
fits_day <- fits_day %>%
  mutate(
    est_regime = case_when(
      conf.low > 0 & conf.high > 0 ~ "Exponential",
      TRUE ~ "Baseline"
    ),
    correct = case_when(
      regime == est_regime ~ TRUE,
      TRUE ~ FALSE
    )
  )

fits_day %>%
  ggplot(aes(x = id, y = estimate, ymin = conf.low, ymax = conf.high, col = correct)) +
    geom_pointrange(alpha = 0.05, size = 0.5) +
    theme_minimal() +
    scale_color_manual(values = c("#D22B2B", "grey")) +
    labs(x = "k-mer ID", y = "Estimated slope", col = "True regime") +
    theme(
      legend.position = "bottom"
    )

fits_day %>%
  ggplot(aes(x = id, y = p.value, col = correct)) +
  geom_point() +
  theme_minimal() +
    scale_color_manual(values = c("#D22B2B", "grey")) +
  labs(x = "k-mer ID", y = "p-value for positive slope", col = "True regime") +
  theme(
    legend.position = "bottom"
  )
```

All of the $k$-mers which are exponentially increasing are classified as such.
In other words, the false negative rate is zero.
There are a small number of false positives: $k$-mers in the baseline regime which are classified as exponentially increasing.

```{r}
fits_day <- fits_day %>%
  mutate(
    cm = case_when(
      regime == "Exponential" & est_regime == "Exponential" ~ "True positive",
      regime == "Exponential" & est_regime == "Baseline" ~ "False negative",
      regime == "Baseline" & est_regime == "Exponential" ~ "False positive",
      regime == "Baseline" & est_regime == "Baseline" ~ "True negative"
    )
  )

fits_day %>%
  janitor::tabyl(cm) %>%
  knitr::kable()
```

We can highlight these few false positives in a plot.

```{r}
df %>%
  left_join(
    select(fits_day, id, correct, cm),
    by = "id"
  ) %>%
  ggplot(aes(x = day, y = count, col = correct, alpha = correct, group = id)) +
    geom_line() + 
    theme_minimal() +
    scale_alpha_manual(values = c(1, 0.05)) +
    scale_color_manual(values = c("#D22B2B", "grey")) +
    guides(alpha = "none") + 
    labs(x = "Day", y = "Number of reads in sample", col = "Correctly classified?")
```

TODO: Compute classification probabilities.

The Brier score (see [Evaluating exponential growth detection](https://docs.google.com/document/d/1wSi6RknvA91mEB7gY8icdcUPHP7kEObhdwXDb5D9FAY/edit)) is given by
$$
\text{BS} = \frac{1}{n} \sum_{i = 1}^n \left(f_i - o_i \right)^2
$$
where $f_i$ is $\mathbb{P} \left[ \text{regime}(i) = 1 \right]$ and $o_i = \text{regime}(i) \in \{0, 1\}$.

### Benchmarking

We can benchmark how long the Poisson regression model takes to fit using the function `bench::mark`:

```{r}
bm <- bench::mark(
  glm(count ~ 1 + day, family = "poisson", data = filter(df, id == 1))
)

data_subset <- filter(df, id == 1)

bm2 <- bench::mark(
  glm(count ~ 1 + day, family = "poisson", data = data_subset)
) %>%
  print()

bm
bm2

saveRDS(bm, "benchmark-sample0.rds")
```

## With additional true abundance noise

```{r}
df <- readRDS("depends/sample1.rds") %>%
  as_tibble()
```

## Real data

```{r, class.source = 'fold-show'}
counts <- read.table("data/counts-1pct-sample.tsv", header = TRUE)

summary(counts)

#' There are this many k-mers in the data
nrow(counts)

#' These are the column names for counts
names(counts)

#' This is what the top part of the data looks like
head(counts)

counts_subset <- filter(counts, ec < 1000)

#' In a tidy format for plotting and fitting
counts_subset_long <- counts_subset %>%
  pivot_longer(
    cols = starts_with("day"),
    names_to = "day",
    names_prefix = "day",
    values_to = "count"
  ) %>%
  mutate(day = as.integer(day))

#' Plot the time series
ggplot(counts_subset_long, aes(x = day, y = count, group = ec)) +
  geom_line(alpha = 0.05) +
  theme_minimal() +
  labs(x = "Day", y = "Count")

#' There is also a file which I assume is the total reads
row_sums <- read.table("data/daily-counts.tsv", header = TRUE)
row_sums
```
## Notes from meeting with Mike

Time per e.g. million ECs might be a useful format for benchmarking.
Time for each of the different subsets.

?glm to check what the possible input formats are.
Benchmarking the different steps (including pivot_longer).
Use data.table to do the pivot.

Create `egd.R` that Jeff would source.
Benchmark aspects of egd.R and try to improve the slowest running parts.
Pivoting (and other aspects) might not be linear.
How does pivoting scale with size?

Look at different aspects (modules) of the simulation (to have technical replicates).
Datasets that have that are not going to be wastewater (but we can also run experiments).
Dirichlet Multinomial maybe not as good.

## Notes on feedback from Sam

Status: short response, to integrate feedback into documents.

### Major comments

* More detail on expected sampling process would help: I assume this refers to the metagenomic sampling process, which I agree is a hole at the moment and something I'm looking to fill in further. For now, I just choose to use the a multinomial sample with number of reads as sample size as it seemed the simplest.
* Some example data would be very useful to understand the process: I'm unsure exactly what "example data" refers to here, would it be real data or more examples of simulated data? Since there are examples of simulated data in there, I'd assume real data. I agree that we need to start looking at more real data (so far I've only looked at one example of real data).
* Rationale for motivating complex simulation model then using Poisson regression: Expecting computation to be restrictive, so using the simplest possible inferential methods for the time being. If it's shown that we can afford more computation, and there are things that Poisson regression doesn't do well, then we could think about patching them up with something more sophisticated.

### Minor comments

* More context on expected processes happening in background: Agree that what has been done is quite abstract and not very linked to a particular scenario or case-study. Since I've been talking about making things realsitic, picking one case-study to focus in on and pick realistic values and settings for could be a good route.
* 1/10 of simulated targets are positive: Yes this is likely too high, we don't want to be flagging 10% of things. This is easier than we expect in reality.
* Constant baseline makes it easier: Agree, this is very easy as things go (you can just eyeball them all).
* What about a structural time-series model with local trends: Yes that would be great. Not only, but it would also highlight a problem we're going to be running into (I assume) in that there will be naturally occuring exponential growth which is not a threat, so knowing exactly how to be dealing with that (or even how often it's the case) is important.
* Maximum sample concentration: Not sure, but one thing is that we expect the sample concentration of the thing we want to detect to be much lower than everything else (which hasn't been captured well by these simulations).
* Gaussian random walk creates large changes: Agree, I've found that draws from Gaussian growth rate with mean greater than zero (exponential regime) can be quite varied. Worth thinking about more.
* Adjustments for prevalence and not incidence: Good point, I don't know. For now we're assuming that there will be an exponential signal in the $k$-mers, which I agree is probably proportional to prevalence more than incidence. I think it's an open question whether we need to be adjusting for this (which potentially more epidemiological modelling [Charlie / Janvi?] could help with)
