---
title: "Exponential growth detection on metagenomic data"
author:
- name: Adam Howes
output:
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
    df_print: paged
    code_folding: hide
    theme: lumen
abstract: |
    **Background** We can test approaches to exponential growth detection using simulated metagenomic time-series data. We have generated this data in the notebook "Simulating metagenomic time series data".
    
    **Task** In this notebook we will import the simulated data, perform and clarify the Poisson regression approach to exponential growth detection, and measure performance against the ground truth using a range of approaches.
    
    **Findings** 
    
    **Next steps** 
---

```{r setup, class.source = 'fold-hide'}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  autodep = TRUE,
  cache.comments = FALSE
)

options(scipen = 999)

cbpalette <- multi.utils::cbpalette()

library(tidyverse)
```

## First simulated example

We start by importing the data generated in [Introduction via simple version, Simulating metagenomic time series data](https://athowes.github.io/exp-growth/metagenomic-time-series#Introduction_via_simple_version).

```{r}
df <- readRDS("depends/sample0.rds") %>%
  as_tibble()
```

A full description of the data is provided in the linked notebook, but to give a quick idea: the plot below shows each draw from the two regimes, baseline and exponential, shown as feint lines, together with corresponding regime 95% intervals shown as shaded region and regime mean shown as bold line. 

```{r}
sample_summary <- df %>%
  group_by(day, regime) %>%
  summarise(
    count_upper = quantile(count, 0.95),
    count_median = median(count),
    count_lower = quantile(count, 0.05)
  ) %>%
  ungroup()

ggplot(sample_summary, aes(x = day, ymin = count_lower, y = count_median, ymax = count_upper, group = regime)) +
    geom_ribbon(alpha = 0.1, aes(fill = regime)) +
    geom_line(aes(col = regime), size = 1.5) +
    geom_line(data = df, aes(x = day, y = count, col = regime, group = id),
               alpha = 0.025, inherit.aes = FALSE) + 
    theme_minimal() +
    scale_color_manual(values = cbpalette) +
    scale_fill_manual(values = cbpalette) +
    guides(fill = "none") +
    labs(x = "Day", y = "Number of reads in sample", col = "Regime")
```

### Performing inference

We fit a Poisson regression model (see [Explaining Poisson regression](https://athowes.github.io/exp-growth/explain-poisson)) or [Estimating exponential growth via sporadic detection](https://athowes.github.io/exp-growth/exponential-detection)) to each $k$-mer in the data.
It is important to note that the generative process used to create the simulated data may be of greater complexity than Poisson regression inferential model.
We take this approach because we expect any functional NAO system to need to be able to handle a large number of $k$-mers (or $k$-mer equivalence classes, etc.), and therefore we require the inferential approach to be highly scalable.
Poisson regression is a comparatively simple inferential approach, which we hope can be shown via benchmarking (see [Benchmarking Poisson regression for exponential growth detection](https://athowes.github.io/exp-growth/egd-benchmark)) to be scalable to the number of $k$-mers we expect, which might be^[To some extent, the number of $k$-mers can be varied in accordance with computational capacity.] on the order of $10^9$.
If Poisson regression is feasible computationally, but has particular unsatisfactory statistical behaviour, then we could consider a more sophisticated inferential model to overcome specific problems.

```{r}
#' Fit to each unique id
#' Note that (confusingly) kmer is not unique, and represents the kmer within a given organism
fits <- tibble(id = unique(df$id)) %>%
  mutate(
    fit = map(id, ~glm(count ~ 1 + day, family = "poisson", data = filter(df, id == .))),
    fit = map(fit, broom::tidy, conf.int = TRUE)
  ) %>%
  unnest(fit)

#' Add truth column
fits <- fits %>%
  left_join(
    select(df, id, regime),
    by = "id"
  )
```

Now we can look at the inference for the slope parameter $\beta$ in each regression, first the point estimate with confidence interval, then the $p$-value for $\beta \neq 0$.
The truth -- either exponential or baseline regime -- is shown by the point colour.

```{r}
fits_day <- filter(fits, term == "day")

fits_day %>%
  ggplot(aes(x = id, y = estimate, ymin = conf.low, ymax = conf.high, col = regime)) +
    geom_pointrange(alpha = 0.05, size = 0.5) +
    theme_minimal() +
    scale_color_manual(values = cbpalette) +
    labs(x = "k-mer ID", y = "Estimated slope", col = "True regime") +
    theme(
      legend.position = "bottom"
    )

fits_day %>%
  ggplot(aes(x = id, y = p.value, col = regime)) +
  geom_point() +
  theme_minimal() +
  scale_color_manual(values = cbpalette) +
  labs(x = "k-mer ID", y = "p-value for positive slope", col = "True regime") +
  theme(
    legend.position = "bottom"
  )

saveRDS(fits, "fits-sample0.rds")
```

### Performance assessment

Suppose we classify as exponential growth when the 95% confidence interval for $\beta$ is above zero.
We can see which $k$-mers we misclassify by comparing this estimated regime to the true regime.

```{r}
fits_day <- fits_day %>%
  mutate(
    est_regime = case_when(
      conf.low > 0 & conf.high > 0 ~ "Exponential",
      TRUE ~ "Baseline"
    ),
    correct = case_when(
      regime == est_regime ~ TRUE,
      TRUE ~ FALSE
    )
  )

fits_day %>%
  ggplot(aes(x = id, y = estimate, ymin = conf.low, ymax = conf.high, col = correct)) +
    geom_pointrange(alpha = 0.05, size = 0.5) +
    theme_minimal() +
    scale_color_manual(values = c("#D22B2B", "grey")) +
    labs(x = "k-mer ID", y = "Estimated slope", col = "True regime") +
    theme(
      legend.position = "bottom"
    )

fits_day %>%
  ggplot(aes(x = id, y = p.value, col = correct)) +
  geom_point() +
  theme_minimal() +
    scale_color_manual(values = c("#D22B2B", "grey")) +
  labs(x = "k-mer ID", y = "p-value for positive slope", col = "True regime") +
  theme(
    legend.position = "bottom"
  )
```

All of the $k$-mers which are exponentially increasing are classified as such.
In other words, the false negative rate is zero.
There are a small number of false positives: $k$-mers in the baseline regime which are classified as exponentially increasing.

```{r}
fits_day <- fits_day %>%
  mutate(
    cm = case_when(
      regime == "Exponential" & est_regime == "Exponential" ~ "True positive",
      regime == "Exponential" & est_regime == "Baseline" ~ "False negative",
      regime == "Baseline" & est_regime == "Exponential" ~ "False positive",
      regime == "Baseline" & est_regime == "Baseline" ~ "True negative"
    )
  )

fits_day %>%
  janitor::tabyl(cm) %>%
  knitr::kable()
```

We can highlight these few false positives in a plot.

```{r}
df %>%
  left_join(
    select(fits_day, id, correct, cm),
    by = "id"
  ) %>%
  ggplot(aes(x = day, y = count, col = correct, alpha = correct, group = id)) +
    geom_line() + 
    theme_minimal() +
    scale_alpha_manual(values = c(1, 0.05)) +
    scale_color_manual(values = c("#D22B2B", "grey")) +
    guides(alpha = "none") + 
    labs(x = "Day", y = "Number of reads in sample", col = "Correctly classified?")
```

TODO: Compute classification probabilities.

The Brier score (see [Evaluating exponential growth detection](https://docs.google.com/document/d/1wSi6RknvA91mEB7gY8icdcUPHP7kEObhdwXDb5D9FAY/edit)) is given by
$$
\text{BS} = \frac{1}{n} \sum_{i = 1}^n \left(f_i - o_i \right)^2
$$
where $f_i$ is $\mathbb{P} \left[ \text{regime}(i) = 1 \right]$ and $o_i = \text{regime}(i) \in \{0, 1\}$.

### Benchmarking

We can benchmark how long the Poisson regression model takes to fit using the function `bench::mark`:

```{r}
bm <- bench::mark(
  glm(count ~ 1 + day, family = "poisson", data = filter(df, id == 1))
)

data_subset <- filter(df, id == 1)

bm2 <- bench::mark(
  glm(count ~ 1 + day, family = "poisson", data = data_subset)
) %>%
  print()

bm
bm2

saveRDS(bm, "benchmark-sample0.rds")
```

## With additional true abundance noise

```{r}
df <- readRDS("depends/sample1.rds") %>%
  as_tibble()
```

## Real data

```{r, class.source = 'fold-show'}
counts <- read.table("data/counts-1pct-sample.tsv", header = TRUE)

summary(counts)

#' There are this many k-mers in the data
nrow(counts)

#' These are the column names for counts
names(counts)

#' This is what the top part of the data looks like
head(counts)

counts_subset <- filter(counts, ec < 1000)

#' In a tidy format for plotting and fitting
counts_subset_long <- counts_subset %>%
  pivot_longer(
    cols = starts_with("day"),
    names_to = "day",
    names_prefix = "day",
    values_to = "count"
  ) %>%
  mutate(day = as.integer(day))

#' Plot the time series
ggplot(counts_subset_long, aes(x = day, y = count, group = ec)) +
  geom_line(alpha = 0.05) +
  theme_minimal() +
  labs(x = "Day", y = "Count")

#' There is also a file which I assume is the total reads
row_sums <- read.table("data/daily-counts.tsv", header = TRUE)
row_sums
```

## Notes on feedback from Sam

Status: short response, some points already integrated into document.

### Major comments

* More detail on expected sampling process would help: I assume this refers to the metagenomic sampling process, which I agree is a hole at the moment and something I'm looking to fill in further. For now, I just choose to use the a multinomial sample with number of reads as sample size as it seemed the simplest.
* Some example data would be very useful to understand the process: I'm unsure exactly what "example data" refers to here, would it be real data or more examples of simulated data? Since there are examples of simulated data in there, I'd assume real data. I agree that we need to start looking at more real data (so far I've only looked at one example of real data).

### Minor comments

* More context on expected processes happening in background: Agree that what has been done is quite abstract and not very linked to a particular scenario or case-study. Since I've been talking about making things realistic, picking one case-study to focus in on and pick realistic values and settings for could be a good route.
* Gaussian random walk creates large changes: Agree, I've found that draws from Gaussian growth rate with mean greater than zero (exponential regime) can be quite varied. Worth thinking about more.
* Adjustments for prevalence and not incidence: Good point, I don't know. For now we're assuming that there will be an exponential signal in the $k$-mers, which I agree is probably proportional to prevalence more than incidence. I think it's an open question whether we need to be adjusting for this (which potentially more epidemiological modelling [Charlie / Janvi?] could help with)
