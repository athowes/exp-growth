---
title: "Simulating metagenomic time series data"
author:
  - Adam Howes
output:
  html_document:
    df_print: paged
---

```{r echo = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  out.width = "95%",
  fig.align = 'center'
)

library(tidyverse)
```

We would like to simulate metagenomic data.
To begin with, we will use a lot of simplifying assumptions to make things easy.

```{r}
n_organism <- 100
n_bp <- 10^3
k <- 40
n_kmer <- n_bp - k
time_window <- 14
baseline <- 100
r <- 1
read_depth <- 10^6
```

Suppose that there are `r n_organism` organisms in total (and nothing else exists).
Each organism has a genome that is exactly `r n_bp` base-pairs long.
Each genome is broken into $k$-mers of length `r k`.
There are `r n_kmer` $k$-mers from each organism, obtained by subtracting the $k$-mer length from the genome size.
Assume that each $k$-mer is unique, and there are no sequencing errors.

We collect data over a period of `r time_window` days $t = 0, 1, \dots$ at a single environmental monitoring site.
Every species has a baseline concentration $c^{(b)}$ in water of `r baseline` copy per $\mu$L^[The units don't matter much.].
Suppose that one of the species is experiencing exponential growth.
It starts at the baseline concentration, but over the `r time_window` day window its concentration increases exponentially according to
$$
c^{(e)}_t = c^{(e)}_0 \exp(rt)
$$
where $r$ is the growth rate which we take to be `r r`.

```{r}
#' The concentration of the baseline organisms over the time window
conc_baseline <- rep(baseline, time_window)
conc_baseline

#' The concentration of the exponentially increasing organism over the time window
conc_exponential <- baseline * exp(r * 0:13)
conc_exponential
```

We will represent the true concentrations of each organism with a matrix `C`, and place the exponentially increasing organism in the first column.

```{r}
C <- matrix(
  data = c(conc_exponential, rep(conc_baseline, n_organism - 1)),
  nrow = time_window,
  ncol = n_organism
)

C[1:14, 1:5]
```

The true concentration of each $k$-mer is that of the corresponding organism.
We can represent this by copying each column of the matrix `C` `r n_bp` times.

```{r}
K <- matrix(rep(as.numeric(t(C)), each = n_kmer), nrow = time_window, byrow = TRUE)
```
The matrix `K` now has `r nrow(K)` rows, one for each day, and `r ncol(K)` columns, one for each $k$-mer (of which there are `r n_kmer` times `r n_organism`).

We can calculate proportions, where the total number of $k$-mers is given by the row sums:

```{r}
K_norm <- t(apply(K, 1, function(x) x / sum(x), simplify = TRUE))
useful::topleft(K_norm)
useful::bottomleft(K_norm)
```

Suppose that the read depth is `r read_depth`.
We take a multinomial sample with probabilities of sampling each $k$-mer given by `K_norm`.

```{r}
#' The proportions from day 1
K_norm_one <- K_norm[1, ]
sample_one <- rmultinom(1, read_depth, K_norm_one)
hist(sample_one)

#' Try to do all of the samples with an apply
sample <- apply(K_norm, 1, function(row) rmultinom(1, read_depth, row))

colnames(sample) <- paste0("day", 1:ncol(sample))
rownames(sample) <- paste0(1:nrow(sample))

sample_df <- sample %>%
  as.data.frame() %>%
  tibble::rownames_to_column("id") %>%
  pivot_longer(
    cols = starts_with("day"),
    names_to = "day",
    values_to = "count",
    names_prefix = "day"
  ) %>%
  mutate(
    id = as.numeric(id),
    day = as.numeric(day),
    kmer = rep(rep(1:n_kmer, each = time_window), times = n_organism),
    organism = rep(1:n_organism, each = n_kmer * time_window)
  )
```

Just plotting the data from the organism which we have set to be exponentially increasing:

```{r}
sample_summary <- sample_df %>%
  filter(organism == 1) %>%
  group_by(day) %>%
  summarise(
    count_upper = quantile(count, 0.95),
    count_median = median(count),
    count_lower = quantile(count, 0.05)
  )

ggplot(sample_summary, aes(x = day, ymin = count_lower, y = count_median, ymax = count_upper)) +
    geom_ribbon(alpha = 0.1) +
    geom_point() +
    geom_point(data = filter(sample_df, organism == 1), aes(x = day, y = count, col = kmer),
               alpha = 0.01, inherit.aes = FALSE) + 
    theme_minimal() +
    labs(x = "Day", y = "Number of reads in sample", col = "kmer")
```

```{r}
pickout_day <- 10
```

With these settings, on day `r pickout_day` the median count of organism 1 is `r filter(sample_summary, day == pickout_day)$count_median`.
At this stage, the exponential growth curve has started to level off because $k$-mers from organism 1 already represent `r 100 * filter(sample_summary, day == pickout_day)$count_median * n_kmer / read_depth`\% of the total reads.
This is unrealistic as it would be unlikely, or for the novel pathogens we are considering essentially impossible, for any one organism to saturate the space.

<!-- hey adam, this looks like a good start to me. Seems ok to start working on the detection side of things. Some ways to make the simulations a bit more realistic/relevant you could experiment with: -->
<!-- * we expect the exponentially increasing viruses we are catching to have a very small initial relative abundance; so you could start this one with a much lower concentration, and then it wonâ€™t saturate over the simulation period -->
<!-- * there is additional noise + bias in the seq measurements which I also would model as lognormal but fine to ignore for now -->

## Adding noise to the true abundances

Suppose now that the organisms have abundances which vary stochastically, rather than either be fixed or increasing exponentially deterministically.

### Baseline

For the baseline concentration, we will use a lognormal geometric random walk such that
$$
c^{(b)}_t = c^{(b)}_{t - 1} \times \exp \left( \epsilon^{(b)}_t \right)
$$
where $\exp(\epsilon^{(b)}_t) \sim \mathcal{N}(0, \sigma_b^2)$.
Another way to calculate $c^{(b)}_t$ is as
$$
c^{(b)}_t = c^{(b)}_{0} \times \exp \left( \sum_{t = 1}^T \epsilon^{(b)}_t \right)
$$
Note that the expected cumulative sum of innovations is always zero, such that the expected concentration at any time is that of $c^{(b)}_{0}$

```{r}
n_baseline_paths <- 8
```

To show how this behaviour looks, let's simulate `r n_baseline_paths` baseline paths.

```{r}
baseline_df <- data.frame(
  day = rep(1:time_window, times = n_baseline_paths), 
  epsilon = rnorm(time_window * n_baseline_paths),
  path_number = as.factor(rep(1:n_baseline_paths, each = time_window))
  ) %>%
  group_by(path_number) %>%
  arrange(path_number) %>%
  mutate(
    cumsum_epsilon = cumsum(epsilon),
    conc = baseline * exp(cumsum_epsilon)
  ) %>%
  ungroup()

cbpalette <- c("#56B4E9", "#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

baseline_df %>%
  pivot_longer(
    cols = c("epsilon", "cumsum_epsilon", "conc"),
    names_to = "variable",
    values_to = "value"
  ) %>%
  mutate(
    variable = recode_factor(variable, 
      "epislon" = "epsilon", 
      "cumsum_epsilon" = "cumsum(epsilon)",
      "conc" = "Concentration")
  ) %>%
  ggplot(aes(x = day, y = value, group = path_number, col = path_number)) +
    geom_line() +
    facet_wrap(~variable, ncol = 1, scales = "free") +
    scale_colour_manual(values = cbpalette) +
    theme_minimal() +
    labs(x = "Day", y = "", col = "Path number", title = "Baseline behaviour")
```

The key takeaway for me is the even though the noise is IID and Gaussian, when you take the cumulative sum and exponentiate it can lead to large deviations in concentration.
For example, the maximum concentration value observed was `r max(baseline_df$conc)` -- which is `r max(baseline_df$conc) / baseline` greater than the baseline concentration.

### Exponential

For the exponential regime, we also suppose a geometric lognormal random walk
$$
c^{(e)}_t = c^{(e)}_{t - 1} \times \exp \left( \epsilon^{(e)}_t \right)
$$
where $\exp(\epsilon^{(e)}_t) \sim \mathcal{N}(r, \sigma_e^2)$ and the growth rate $r > 0$.
The expected concentration is $c^{(b)}_{0} \times \exp(rt)$.
Let's simulate some paths from this distribution as before.

```{r}
exponential_df <- data.frame(
  day = rep(1:time_window, times = n_baseline_paths), 
  epsilon = rnorm(time_window * n_baseline_paths, mean = r),
  path_number = as.factor(rep(1:n_baseline_paths, each = time_window))
  ) %>%
  group_by(path_number) %>%
  arrange(path_number) %>%
  mutate(
    cumsum_epsilon = cumsum(epsilon),
    conc = baseline * exp(cumsum_epsilon)
  ) %>%
  ungroup()

exponential_df %>%
  pivot_longer(
    cols = c("epsilon", "cumsum_epsilon", "conc"),
    names_to = "variable",
    values_to = "value"
  ) %>%
  mutate(
    variable = recode_factor(variable, 
      "epislon" = "epsilon", 
      "cumsum_epsilon" = "cumsum(epsilon)",
      "conc" = "Concentration")
  ) %>%
  ggplot(aes(x = day, y = value, group = path_number, col = path_number)) +
    geom_line() +
    facet_wrap(~variable, ncol = 1, scales = "free") +
    scale_colour_manual(values = cbpalette) +
    theme_minimal() +
    labs(x = "Day", y = "", col = "Path number", title = "Exponential behaviour")
```

## Observation models

* See: https://arxiv.org/pdf/2001.04343.pdf

### Dirichlet-multinomial model

The Dirichlet is a probability distribution over proportions. 
$w \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_K)$ if
$$
p(w; \alpha) = \frac{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma(\alpha_k)} w_1^{\alpha_1 - 1} w_2^{\alpha_2 - 1} \times \cdots \times w_K^{\alpha_K - 1}
$$
```{r}
dirichlet_draw <- gtools::rdirichlet(n = 1, alpha = K_norm_one)
```


### Poisson-lognormal model

* See: https://pln-team.github.io/PLNmodels/

The multivariate Poisson-lognormal model (MPLN) is set-up for analysis of an abunance table, an $n \times p$ matrix $Y$ where $Y_{ij}$ is the number of individuals from species $j \leq p$ observed at site $i \leq n$.
"Species" could refer to an operational taxonomic unit (OTU) or amplicon sequence variant (ASV), "site" could refer to a sample or experiment, and "number of individuals" could refer to number of reads.
One model is
$$
Y_{ij} \, | \, Z_{ij} \sim \text{Poisson}(\exp(o_{ij} + Z_{ij})), \\
Z_i \sim \mathcal{N}(\mu_i, \Sigma),
$$
where $Z_i$ are assumed to be independent across sites.
Some moments of $Y$ are given by:
$$
\mathbb{E}(Y_{ij}) = \exp(o_{ij} + \mu_{ij} + \sigma_{jj} / 2) > 0 \\
\mathbb{V}(Y_{ij}) = \mathbb{E}(Y_{ij}) + \mathbb{E}(Y_{ij})^2 (\exp(\sigma_{jj} - 1)) > \mathbb{E}(Y_{ij}) \\
\mathbb{Cov}(Y_{ij}, Y_{ik}) = \mathbb{E}(Y_{ij}) \mathbb{E}(Y_{ik}) (\exp(\sigma_{jk}) - 1)).
$$
Note:
1. Expected count is greater than just adding the exponential of the mean of the latent layer to the expected log abundances because of the logarithmic link function.
2. The model displays over-dispersion as compared with a Poisson model where the variance is the same as the mean, because of the latent noise. (I'm still interested as to how this is different from e.g. a negative-binomial model, though I prefer to use random effects for noise as they are easier to modify and extend to include structure say [amongst other reasons] than likelihoods).
3. "Faithful correlation" in that the correlation and covariance have the same sign. As well, if $\sigma_{jk} = 0$ then $\mathbb{Cov}(Y_{ij}, Y_{ik}) = 0$.
