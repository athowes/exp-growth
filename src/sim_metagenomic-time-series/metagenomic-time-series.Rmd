---
title: "Simulating metagenomic time series data"
author:
  - Adam Howes
output:
  html_document:
    df_print: paged
---

```{r echo = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 320,
  out.width = "95%",
  fig.align = 'center'
)

library(tidyverse)
```

We would like to simulate metagenomic data.
To begin with, we will use a lot of simplifying assumptions to make things easy.

```{r}
n_organism <- 100
n_bp <- 10^3
k <- 40
n_kmer <- n_bp - k
time_window <- 14
baseline <- 100
r <- 1
read_depth <- 10^6
```

Suppose that there are `r n_organism` organisms in total (and nothing else exists).
Each organism has a genome that is exactly `r n_bp` base-pairs long.
Each genome is broken into $k$-mers of length `r k`.
There are `r n_kmer` $k$-mers from each organism, obtained by subtracting the $k$-mer length from the genome size.
Assume that each $k$-mer is unique, and there are no sequencing errors.

We collect data over a period of `r time_window` days $t = 0, 1, \dots$ at a single environmental monitoring site.
Every species has a baseline concentration $c^{(b)}$ in water of `r baseline` copy per $\mu$L^[The units don't matter much.].
Suppose that one of the species is experiencing exponential growth.
It starts at the baseline concentration, but over the `r time_window` day window its concentration increases exponentially according to
$$
c^{(e)}_t = c^{(e)}_0 \exp(rt)
$$
where $r$ is the growth rate which we take to be `r r`.

```{r}
#' The concentration of the baseline organisms over the time window
conc_baseline <- rep(baseline, time_window)
conc_baseline

#' The concentration of the exponentially increasing organism over the time window
conc_exponential <- baseline * exp(r * 0:13)
conc_exponential
```

We will represent the true concentrations of each organism with a matrix `C`, and place the exponentially increasing organism in the first column.

```{r}
C <- matrix(
  data = c(conc_exponential, rep(conc_baseline, n_organism - 1)),
  nrow = time_window,
  ncol = n_organism
)

C[1:14, 1:5]
```

The true concentration of each $k$-mer is that of the corresponding organism.
We can represent this by copying each column of the matrix `C` `r n_bp` times.

```{r}
K <- matrix(rep(as.numeric(t(C)), each = n_kmer), nrow = time_window, byrow = TRUE)
```
The matrix `K` now has `r nrow(K)` rows, one for each day, and `r ncol(K)` columns, one for each $k$-mer (of which there are `r n_kmer` times `r n_organism`).

We can calculate proportions, where the total number of $k$-mers is given by the row sums:

```{r}
K_norm <- t(apply(K, 1, function(x) x / sum(x), simplify = TRUE))
useful::topleft(K_norm)
useful::bottomleft(K_norm)
```

Suppose that the read depth is `r read_depth`.
We take a multinomial sample with probabilities of sampling each $k$-mer given by `K_norm`.

```{r}
#' The proportions from day 1
K_norm_one <- K_norm[1, ]
sample_one <- rmultinom(1, read_depth, K_norm_one)
hist(sample_one)

#' Try to do all of the samples with an apply
sample <- apply(K_norm, 1, function(row) rmultinom(1, read_depth, row))

colnames(sample) <- paste0("day", 1:ncol(sample))
rownames(sample) <- paste0(1:nrow(sample))

sample_df <- sample %>%
  as.data.frame() %>%
  tibble::rownames_to_column("id") %>%
  pivot_longer(
    cols = starts_with("day"),
    names_to = "day",
    values_to = "count",
    names_prefix = "day"
  ) %>%
  mutate(
    id = as.numeric(id),
    day = as.numeric(day),
    kmer = rep(rep(1:n_kmer, each = time_window), times = n_organism),
    organism = rep(1:n_organism, each = n_kmer * time_window)
  )
```

Just plotting the data from the organism which we have set to be exponentially increasing:

```{r}
sample_summary <- sample_df %>%
  filter(organism == 1) %>%
  group_by(day) %>%
  summarise(
    count_upper = quantile(count, 0.95),
    count_median = median(count),
    count_lower = quantile(count, 0.05)
  )

ggplot(sample_summary, aes(x = day, ymin = count_lower, y = count_median, ymax = count_upper)) +
    geom_ribbon(alpha = 0.1) +
    geom_point() +
    geom_point(data = filter(sample_df, organism == 1), aes(x = day, y = count, col = kmer),
               alpha = 0.01, inherit.aes = FALSE) + 
    theme_minimal() +
    labs(x = "Day", y = "Number of reads in sample", col = "kmer")
```

```{r}
pickout_day <- 10
```

With these settings, on day `r pickout_day` the median count of organism 1 is `r filter(sample_summary, day == pickout_day)$count_median`.
At this stage, the exponential growth curve has started to level off because $k$-mers from organism 1 already represent `r 100 * filter(sample_summary, day == pickout_day)$count_median * n_kmer / read_depth`\% of the total reads.
This is unrealistic as it would be unlikely, or for the novel pathogens we are considering essentially impossible, for any one organism to saturate the space.

<!-- hey adam, this looks like a good start to me. Seems ok to start working on the detection side of things. Some ways to make the simulations a bit more realistic/relevant you could experiment with: -->
<!-- * we expect the exponentially increasing viruses we are catching to have a very small initial relative abundance; so you could start this one with a much lower concentration, and then it wonâ€™t saturate over the simulation period -->
<!-- * we expect there to be noise in the true abundances; e.g. every day, the exponential increase factor for all organisms might have a lognormal distribution; for the baseline orgs, this could have a geometric mean of 1, but for the increasing org/threat the gm mean would be >1 -->
<!-- * there is additional noise + bias in the seq measurements which I also would model as lognormal but fine to ignore for now -->

## Adding noise to the true abundances

Suppose now that the organisms have abundances which vary stochastically, rather than either be fixed or increasing exponentially deterministically.

## Baseline

For the baseline concentration, we will use a lognormal geometric random walk such that
$$
c^{(b)}_t = c^{(b)}_{t - 1} \times \exp \left( \epsilon^{(b)}_t \right)
$$
where $\exp(\epsilon^{(b)}_t) \sim \mathcal{N}(0, \sigma_b^2)$.
Another way to calculate $c^{(b)}_t$ is as
$$
c^{(b)}_t = c^{(b)}_{0} \times \exp \left( \sum_{t = 1}^T \epsilon^{(b)}_t \right)
$$
Note that the expected cumulative sum of innovations is always zero, such that the expected concentration at any time is that of $c^{(b)}_{0}$

```{r}
n_baseline_paths <- 8
```

To show how this behaviour looks, let's simulate `r n_baseline_paths` baseline paths.

```{r}
baseline_df <- data.frame(
  day = rep(1:time_window, times = n_baseline_paths), 
  epsilon = rnorm(time_window * n_baseline_paths),
  path_number = as.factor(rep(1:n_baseline_paths, each = time_window))
  ) %>%
  group_by(path_number) %>%
  arrange(path_number) %>%
  mutate(
    cumsum_epsilon = cumsum(epsilon),
    conc = baseline * exp(cumsum_epsilon)
  ) %>%
  ungroup()

cbpalette <- c("#56B4E9", "#009E73", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

baseline_df %>%
  pivot_longer(
    cols = c("epsilon", "cumsum_epsilon", "conc"),
    names_to = "variable",
    values_to = "value"
  ) %>%
  mutate(
    variable = recode_factor(variable, 
      "epislon" = "epsilon", 
      "cumsum_epsilon" = "cumsum(epsilon)",
      "conc" = "Concentration")
  ) %>%
  ggplot(aes(x = day, y = value, group = path_number, col = path_number)) +
    geom_line() +
    facet_wrap(~variable, ncol = 1, scales = "free") +
    scale_colour_manual(values = cbpalette) +
    theme_minimal() +
    labs(x = "Day", y = "", col = "Path number")
```

The key takeaway for me is the even though the noise is IID and Gaussian, when you take the cumulative sum and exponentiate it can lead to large deviations in concentration.
For example, the maximum concentration value observed was `r max(baseline_df$conc)` -- which is `r max(baseline_df$conc) / baseline` greater than the baseline concentration. 
